{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6986c5-2e2a-4692-8644-ad2da9a683f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3,org.apache.kafka:kafka-clients:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the stream from Kafka\n",
    "kafka_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"dcgm-metrics\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the Kafka message to string\n",
    "processed_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "# Output the processed dataframe to the console\n",
    "query = processed_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Wait for the termination of the streaming query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edec7721-2f7f-4d0d-b971-76c7003340a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation for 8 nodes using 10 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-23 14:44:05 +0530] [31989] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31990] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31989] [INFO] Listening at: http://0.0.0.0:51000 (31989)\n",
      "[2024-10-23 14:44:05 +0530] [31991] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31989] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [31994] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [32005] [INFO] Booting worker with pid: 32005\n",
      "[2024-10-23 14:44:05 +0530] [31990] [INFO] Listening at: http://0.0.0.0:51001 (31990)\n",
      "[2024-10-23 14:44:05 +0530] [31999] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31990] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32004] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [32010] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31994] [INFO] Listening at: http://0.0.0.0:51003 (31994)\n",
      "[2024-10-23 14:44:05 +0530] [32020] [INFO] Booting worker with pid: 32020\n",
      "[2024-10-23 14:44:05 +0530] [31994] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32017] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [31999] [INFO] Listening at: http://0.0.0.0:51004 (31999)\n",
      "[2024-10-23 14:44:05 +0530] [32033] [INFO] Booting worker with pid: 32033\n",
      "[2024-10-23 14:44:05 +0530] [31999] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32023] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [32030] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-23 14:44:05 +0530] [32042] [INFO] Booting worker with pid: 32042\n",
      "[2024-10-23 14:44:05 +0530] [32017] [INFO] Listening at: http://0.0.0.0:51007 (32017)\n",
      "[2024-10-23 14:44:05 +0530] [32017] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32051] [INFO] Booting worker with pid: 32051\n",
      "[2024-10-23 14:44:05 +0530] [32030] [INFO] Listening at: http://0.0.0.0:51009 (32030)\n",
      "[2024-10-23 14:44:05 +0530] [32030] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32058] [INFO] Booting worker with pid: 32058\n",
      "[2024-10-23 14:44:05 +0530] [32023] [INFO] Listening at: http://0.0.0.0:51008 (32023)\n",
      "[2024-10-23 14:44:05 +0530] [32023] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32065] [INFO] Booting worker with pid: 32065\n",
      "[2024-10-23 14:44:05 +0530] [32010] [INFO] Listening at: http://0.0.0.0:51006 (32010)\n",
      "[2024-10-23 14:44:05 +0530] [32010] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32072] [INFO] Booting worker with pid: 32072\n",
      "[2024-10-23 14:44:05 +0530] [32077] [INFO] Booting worker with pid: 32077\n",
      "[2024-10-23 14:44:05 +0530] [32075] [INFO] Booting worker with pid: 32075\n",
      "[2024-10-23 14:44:05 +0530] [32076] [INFO] Booting worker with pid: 32076\n",
      "[2024-10-23 14:44:05 +0530] [32084] [INFO] Booting worker with pid: 32084\n",
      "[2024-10-23 14:44:05 +0530] [32004] [INFO] Listening at: http://0.0.0.0:51005 (32004)\n",
      "[2024-10-23 14:44:05 +0530] [32004] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32085] [INFO] Booting worker with pid: 32085\n",
      "[2024-10-23 14:44:05 +0530] [32090] [INFO] Booting worker with pid: 32090\n",
      "[2024-10-23 14:44:05 +0530] [32097] [INFO] Booting worker with pid: 32097\n",
      "[2024-10-23 14:44:05 +0530] [31991] [INFO] Listening at: http://0.0.0.0:51002 (31991)\n",
      "[2024-10-23 14:44:05 +0530] [32100] [INFO] Booting worker with pid: 32100\n",
      "[2024-10-23 14:44:05 +0530] [31991] [INFO] Using worker: sync\n",
      "[2024-10-23 14:44:05 +0530] [32107] [INFO] Booting worker with pid: 32107\n",
      "[2024-10-23 14:44:05 +0530] [32110] [INFO] Booting worker with pid: 32110\n",
      "[2024-10-23 14:44:05 +0530] [32111] [INFO] Booting worker with pid: 32111\n",
      "[2024-10-23 14:44:05 +0530] [32112] [INFO] Booting worker with pid: 32112\n",
      "[2024-10-23 14:44:05 +0530] [32119] [INFO] Booting worker with pid: 32119\n",
      "[2024-10-23 14:44:05 +0530] [32122] [INFO] Booting worker with pid: 32122\n",
      "[2024-10-23 14:44:05 +0530] [32125] [INFO] Booting worker with pid: 32125\n",
      "[2024-10-23 14:44:05 +0530] [32126] [INFO] Booting worker with pid: 32126\n",
      "[2024-10-23 14:44:05 +0530] [32131] [INFO] Booting worker with pid: 32131\n",
      "[2024-10-23 14:44:05 +0530] [32134] [INFO] Booting worker with pid: 32134\n",
      "[2024-10-23 14:44:05 +0530] [32137] [INFO] Booting worker with pid: 32137\n",
      "[2024-10-23 14:44:05 +0530] [32140] [INFO] Booting worker with pid: 32140\n",
      "[2024-10-23 14:44:05 +0530] [32143] [INFO] Booting worker with pid: 32143\n",
      "[2024-10-23 14:44:05 +0530] [32144] [INFO] Booting worker with pid: 32144\n",
      "[2024-10-23 14:44:05 +0530] [32149] [INFO] Booting worker with pid: 32149\n",
      "[2024-10-23 14:44:05 +0530] [32150] [INFO] Booting worker with pid: 32150\n",
      "[2024-10-23 14:44:05 +0530] [32155] [INFO] Booting worker with pid: 32155\n",
      "[2024-10-23 14:44:05 +0530] [32158] [INFO] Booting worker with pid: 32158\n",
      "[2024-10-23 14:44:05 +0530] [32161] [INFO] Booting worker with pid: 32161\n",
      "[2024-10-23 14:44:05 +0530] [32164] [INFO] Booting worker with pid: 32164\n",
      "[2024-10-23 14:44:05 +0530] [32165] [INFO] Booting worker with pid: 32165\n",
      "[2024-10-23 14:44:05 +0530] [32170] [INFO] Booting worker with pid: 32170\n",
      "[2024-10-23 14:44:05 +0530] [32173] [INFO] Booting worker with pid: 32173\n",
      "[2024-10-23 14:44:05 +0530] [32176] [INFO] Booting worker with pid: 32176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-23 14:45:01 +0530] [31999] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [32030] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [32023] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [31989] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [31994] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [32004] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [32017] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [31990] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [31991] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:01 +0530] [32010] [INFO] Handling signal: term\n",
      "[2024-10-23 14:45:02 +0530] [32164] [INFO] Worker exiting (pid: 32164)\n",
      "[2024-10-23 14:45:02 +0530] [32155] [INFO] Worker exiting (pid: 32155)\n",
      "[2024-10-23 14:45:02 +0530] [32134] [INFO] Worker exiting (pid: 32134)\n",
      "[2024-10-23 14:45:02 +0530] [32084] [INFO] Worker exiting (pid: 32084)\n",
      "[2024-10-23 14:45:02 +0530] [32176] [INFO] Worker exiting (pid: 32176)\n",
      "[2024-10-23 14:45:02 +0530] [32085] [INFO] Worker exiting (pid: 32085)\n",
      "[2024-10-23 14:45:02 +0530] [32065] [INFO] Worker exiting (pid: 32065)\n",
      "[2024-10-23 14:45:02 +0530] [32150] [INFO] Worker exiting (pid: 32150)\n",
      "[2024-10-23 14:45:02 +0530] [32161] [INFO] Worker exiting (pid: 32161)\n",
      "[2024-10-23 14:45:02 +0530] [32097] [INFO] Worker exiting (pid: 32097)\n",
      "[2024-10-23 14:45:02 +0530] [32119] [INFO] Worker exiting (pid: 32119)\n",
      "[2024-10-23 14:45:02 +0530] [32122] [INFO] Worker exiting (pid: 32122)\n",
      "[2024-10-23 14:45:02 +0530] [32005] [INFO] Worker exiting (pid: 32005)\n",
      "[2024-10-23 14:45:02 +0530] [32033] [INFO] Worker exiting (pid: 32033)\n",
      "[2024-10-23 14:45:02 +0530] [32100] [INFO] Worker exiting (pid: 32100)\n",
      "[2024-10-23 14:45:02 +0530] [32111] [INFO] Worker exiting (pid: 32111)\n",
      "[2024-10-23 14:45:02 +0530] [32137] [INFO] Worker exiting (pid: 32137)\n",
      "[2024-10-23 14:45:02 +0530] [32112] [INFO] Worker exiting (pid: 32112)\n",
      "[2024-10-23 14:45:02 +0530] [32149] [INFO] Worker exiting (pid: 32149)\n",
      "[2024-10-23 14:45:02 +0530] [32051] [INFO] Worker exiting (pid: 32051)\n",
      "[2024-10-23 14:45:02 +0530] [32077] [INFO] Worker exiting (pid: 32077)\n",
      "[2024-10-23 14:45:02 +0530] [32058] [INFO] Worker exiting (pid: 32058)\n",
      "[2024-10-23 14:45:02 +0530] [32143] [INFO] Worker exiting (pid: 32143)\n",
      "[2024-10-23 14:45:02 +0530] [32125] [INFO] Worker exiting (pid: 32125)\n",
      "[2024-10-23 14:45:02 +0530] [32107] [INFO] Worker exiting (pid: 32107)\n",
      "[2024-10-23 14:45:02 +0530] [32126] [INFO] Worker exiting (pid: 32126)\n",
      "[2024-10-23 14:45:02 +0530] [32144] [INFO] Worker exiting (pid: 32144)\n",
      "[2024-10-23 14:45:02 +0530] [32131] [INFO] Worker exiting (pid: 32131)\n",
      "[2024-10-23 14:45:02 +0530] [32072] [INFO] Worker exiting (pid: 32072)\n",
      "[2024-10-23 14:45:02 +0530] [32140] [INFO] Worker exiting (pid: 32140)\n",
      "[2024-10-23 14:45:02 +0530] [32075] [INFO] Worker exiting (pid: 32075)\n",
      "[2024-10-23 14:45:02 +0530] [32076] [INFO] Worker exiting (pid: 32076)\n",
      "[2024-10-23 14:45:02 +0530] [32158] [INFO] Worker exiting (pid: 32158)\n",
      "[2024-10-23 14:45:02 +0530] [32173] [INFO] Worker exiting (pid: 32173)\n",
      "[2024-10-23 14:45:02 +0530] [32110] [INFO] Worker exiting (pid: 32110)\n",
      "[2024-10-23 14:45:02 +0530] [32170] [INFO] Worker exiting (pid: 32170)\n",
      "[2024-10-23 14:45:02 +0530] [32020] [INFO] Worker exiting (pid: 32020)\n",
      "[2024-10-23 14:45:02 +0530] [32090] [INFO] Worker exiting (pid: 32090)\n",
      "[2024-10-23 14:45:02 +0530] [32165] [INFO] Worker exiting (pid: 32165)\n",
      "[2024-10-23 14:45:02 +0530] [32042] [INFO] Worker exiting (pid: 32042)\n",
      "[2024-10-23 14:45:02 +0530] [31989] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [32030] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [31990] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [31994] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [31999] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [32004] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [32023] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [32010] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [31991] [INFO] Shutting down: Master\n",
      "[2024-10-23 14:45:02 +0530] [32017] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response\n",
    "import secrets\n",
    "import time\n",
    "import math\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Queue\n",
    "from gunicorn.app.base import BaseApplication\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "NUM_NODES = 8  # Increased to 5096\n",
    "GPUS_PER_NODE = 4\n",
    "GPU_MODEL = 'Tesla V100-SXM2-16GB'\n",
    "GPU_SPECS = {\n",
    "    'sm_clock': (1230, 1380),\n",
    "    'mem_clock': 877,\n",
    "    'power': (300, 350),\n",
    "    'memory': 16384\n",
    "}\n",
    "\n",
    "class GPUSimulator:\n",
    "    def __init__(self, node_id, gpu_id):\n",
    "        self.node_id = node_id\n",
    "        self.gpu_id = gpu_id\n",
    "        self.uuid = f\"GPU-{secrets.token_hex(16)}\"\n",
    "        self.pci_bus_id = f\"{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}.0\"\n",
    "        self.driver_version = \"450.51.06\"\n",
    "        self.start_time = time.time()\n",
    "        self.workload_phase = secrets.randbelow(600) / 600\n",
    "\n",
    "    def simulate_metrics(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        self.workload_phase = ((elapsed_time / 60 + self.workload_phase) % 10) / 10\n",
    "        workload = (math.sin(self.workload_phase * 2 * math.pi) + 1) / 2\n",
    "        workload = min(1, max(0, workload + (secrets.randbelow(20) - 10) / 100))\n",
    "        sm_clock = int(GPU_SPECS['sm_clock'][0] + (GPU_SPECS['sm_clock'][1] - GPU_SPECS['sm_clock'][0]) * workload)\n",
    "        gpu_temp = 30 + int(55 * workload)\n",
    "        power_usage = GPU_SPECS['power'][0] + (GPU_SPECS['power'][1] - GPU_SPECS['power'][0]) * workload\n",
    "        gpu_util = int(100 * workload)\n",
    "        mem_util = int(90 * workload)\n",
    "        fb_used = GPU_SPECS['memory'] * mem_util / 100\n",
    "        fb_free = GPU_SPECS['memory'] - fb_used\n",
    "        return [\n",
    "            ('DCGM_FI_DEV_SM_CLOCK', sm_clock, 'SM clock frequency (in MHz)'),\n",
    "            ('DCGM_FI_DEV_GPU_TEMP', gpu_temp, 'GPU temperature (in C)'),\n",
    "            ('DCGM_FI_DEV_POWER_USAGE', round(power_usage, 2), 'Power draw (in W)'),\n",
    "            ('DCGM_FI_DEV_GPU_UTIL', gpu_util, 'GPU utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_MEM_COPY_UTIL', mem_util, 'Memory utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_FB_FREE', round(fb_free, 2), 'Frame buffer memory free (in MB)'),\n",
    "            ('DCGM_FI_DEV_FB_USED', round(fb_used, 2), 'Frame buffer memory used (in MB)')\n",
    "        ]\n",
    "\n",
    "def generate_gpu_metrics(node_id, gpu_simulator):\n",
    "    metrics = gpu_simulator.simulate_metrics()\n",
    "    formatted_metrics = []\n",
    "    for name, value, help_text in metrics:\n",
    "        formatted_metrics.extend([\n",
    "            f'# HELP {name} {help_text}',\n",
    "            f'# TYPE {name} gauge',\n",
    "            f'{name}{{node=\"{node_id}\",gpu=\"{gpu_simulator.gpu_id}\",UUID=\"{gpu_simulator.uuid}\",'\n",
    "            f'pci_bus_id=\"{gpu_simulator.pci_bus_id}\",device=\"nvidia{gpu_simulator.gpu_id}\",'\n",
    "            f'modelName=\"{GPU_MODEL}\",Hostname=\"node{node_id:04d}\",'\n",
    "            f'DCGM_FI_DRIVER_VERSION=\"{gpu_simulator.driver_version}\"}} {value}'\n",
    "        ])\n",
    "    return '\\n'.join(formatted_metrics)\n",
    "\n",
    "def get_gpu_metrics(node_id):\n",
    "    gpu_simulators = [GPUSimulator(node_id, gpu_id) for gpu_id in range(GPUS_PER_NODE)]\n",
    "    all_metrics = []\n",
    "    for gpu_simulator in gpu_simulators:\n",
    "        all_metrics.append(generate_gpu_metrics(node_id, gpu_simulator))\n",
    "    return '\\n'.join(all_metrics)\n",
    "\n",
    "def worker(node_range, queue):\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/metrics/<int:node_id>', methods=['GET'])\n",
    "    def metrics(node_id):\n",
    "        if node_id in node_range:\n",
    "            metrics_data = get_gpu_metrics(node_id)\n",
    "            return Response(metrics_data, mimetype='text/plain')\n",
    "        return \"Node not found\", 404\n",
    "\n",
    "    class FlaskApplication(BaseApplication):\n",
    "        def __init__(self, app, options=None):\n",
    "            self.application = app\n",
    "            self.options = options or {}\n",
    "            super().__init__()\n",
    "\n",
    "        def load_config(self):\n",
    "            for key, value in self.options.items():\n",
    "                if key in self.cfg.settings and value is not None:\n",
    "                    self.cfg.set(key, value)\n",
    "\n",
    "        def load(self):\n",
    "            return self.application\n",
    "\n",
    "    options = {\n",
    "        'bind': f'0.0.0.0:{queue.get()}',\n",
    "        'workers': 4,\n",
    "    }\n",
    "    FlaskApplication(app, options).run()\n",
    "\n",
    "def main():\n",
    "    num_processes = min(multiprocessing.cpu_count(), 16)  # Limit to 16 processes\n",
    "    nodes_per_process = NUM_NODES // num_processes\n",
    "    remaining_nodes = NUM_NODES % num_processes\n",
    "\n",
    "    processes = []\n",
    "    queue = Queue()\n",
    "\n",
    "    print(f\"Starting simulation for {NUM_NODES} nodes using {num_processes} processes...\")\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        start_node = i * nodes_per_process\n",
    "        end_node = start_node + nodes_per_process\n",
    "        if i == num_processes - 1:\n",
    "            end_node += remaining_nodes\n",
    "        node_range = range(start_node, end_node)\n",
    "        port = 51000 + i\n",
    "        queue.put(port)\n",
    "        p = Process(target=worker, args=(node_range, queue))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    try:\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Simulation stopped.\")\n",
    "        for p in processes:\n",
    "            p.terminate()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675a0077-03c6-47d5-9bdb-05a1a799aa4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 268\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    267\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m  \u001b[38;5;66;03m# Or whatever your desired node count is\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Call main directly\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 261\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop() \u001b[38;5;66;03m# Get the *current* loop (Gunicorn's)\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Run within Gunicorn's loop\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# No need to close the loop here; Gunicorn manages it\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/base_events.py:663\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 663\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    666\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/base_events.py:622\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 622\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    625\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "p = multiprocessing.Process(target=run_simulator, args=(node_range, port))\n",
    "        simulator_processes.append(p)\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84a9f2d-78b7-47ec-a116-c5a33699fffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 359\u001b[0m\n\u001b[1;32m    356\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_nodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessages_per_second\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m messages/second\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import aiohttp\n",
    "import uuid\n",
    "import time\n",
    "import secrets\n",
    "import math\n",
    "import multiprocessing\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "from datetime import datetime, timedelta\n",
    "from flask import Flask, Response\n",
    "from gunicorn.app.base import BaseApplication\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['10.180.8.24:9092', '10.180.8.24:9093', '10.180.8.24:9094']\n",
    "DCGM_KAFKA_TOPIC = 'dcgm-metrics-test'\n",
    "FETCH_INTERVAL = 1  # Seconds\n",
    "RETRIES = 3\n",
    "RETRY_DELAY = 5  # Seconds\n",
    "MAX_NODES = 5096\n",
    "GPUS_PER_NODE = 4\n",
    "GPU_MODEL = 'Tesla V100-SXM2-16GB'\n",
    "GPU_SPECS = {\n",
    "    'sm_clock': (1230, 1380),\n",
    "    'mem_clock': 877,\n",
    "    'power': (300, 350),\n",
    "    'memory': 16384\n",
    "}\n",
    "BENCHMARK_DURATION = 600  # 10 minutes in seconds\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GPUSimulator:\n",
    "    def __init__(self, node_id, gpu_id):\n",
    "        self.node_id = node_id\n",
    "        self.gpu_id = gpu_id\n",
    "        self.uuid = f\"GPU-{secrets.token_hex(16)}\"\n",
    "        self.pci_bus_id = f\"{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}.0\"\n",
    "        self.driver_version = \"450.51.06\"\n",
    "        self.start_time = time.time()\n",
    "        self.workload_phase = secrets.randbelow(600) / 600\n",
    "\n",
    "    def simulate_metrics(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        self.workload_phase = ((elapsed_time / 60 + self.workload_phase) % 10) / 10\n",
    "        workload = (math.sin(self.workload_phase * 2 * math.pi) + 1) / 2\n",
    "        workload = min(1, max(0, workload + (secrets.randbelow(20) - 10) / 100))\n",
    "        sm_clock = int(GPU_SPECS['sm_clock'][0] + (GPU_SPECS['sm_clock'][1] - GPU_SPECS['sm_clock'][0]) * workload)\n",
    "        gpu_temp = 30 + int(55 * workload)\n",
    "        power_usage = GPU_SPECS['power'][0] + (GPU_SPECS['power'][1] - GPU_SPECS['power'][0]) * workload\n",
    "        gpu_util = int(100 * workload)\n",
    "        mem_util = int(90 * workload)\n",
    "        fb_used = GPU_SPECS['memory'] * mem_util / 100\n",
    "        fb_free = GPU_SPECS['memory'] - fb_used\n",
    "        return [\n",
    "            ('DCGM_FI_DEV_SM_CLOCK', sm_clock, 'SM clock frequency (in MHz)'),\n",
    "            ('DCGM_FI_DEV_GPU_TEMP', gpu_temp, 'GPU temperature (in C)'),\n",
    "            ('DCGM_FI_DEV_POWER_USAGE', round(power_usage, 2), 'Power draw (in W)'),\n",
    "            ('DCGM_FI_DEV_GPU_UTIL', gpu_util, 'GPU utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_MEM_COPY_UTIL', mem_util, 'Memory utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_FB_FREE', round(fb_free, 2), 'Frame buffer memory free (in MB)'),\n",
    "            ('DCGM_FI_DEV_FB_USED', round(fb_used, 2), 'Frame buffer memory used (in MB)')\n",
    "        ]\n",
    "\n",
    "def generate_gpu_metrics(node_id, gpu_simulator):\n",
    "    metrics = gpu_simulator.simulate_metrics()\n",
    "    formatted_metrics = []\n",
    "    for name, value, help_text in metrics:\n",
    "        formatted_metrics.extend([\n",
    "            f'# HELP {name} {help_text}',\n",
    "            f'# TYPE {name} gauge',\n",
    "            f'{name}{{node=\"{node_id}\",gpu=\"{gpu_simulator.gpu_id}\",UUID=\"{gpu_simulator.uuid}\",'\n",
    "            f'pci_bus_id=\"{gpu_simulator.pci_bus_id}\",device=\"nvidia{gpu_simulator.gpu_id}\",'\n",
    "            f'modelName=\"{GPU_MODEL}\",Hostname=\"node{node_id:04d}\",'\n",
    "            f'DCGM_FI_DRIVER_VERSION=\"{gpu_simulator.driver_version}\"}} {value}'\n",
    "        ])\n",
    "    return '\\n'.join(formatted_metrics)\n",
    "\n",
    "def get_gpu_metrics(node_id):\n",
    "    gpu_simulators = [GPUSimulator(node_id, gpu_id) for gpu_id in range(GPUS_PER_NODE)]\n",
    "    all_metrics = []\n",
    "    for gpu_simulator in gpu_simulators:\n",
    "        all_metrics.append(generate_gpu_metrics(node_id, gpu_simulator))\n",
    "    return '\\n'.join(all_metrics)\n",
    "\n",
    "def create_flask_app(node_range):\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/metrics/<int:node_id>', methods=['GET'])\n",
    "    def metrics(node_id):\n",
    "        if node_id in node_range:\n",
    "            metrics_data = get_gpu_metrics(node_id)\n",
    "            return Response(metrics_data, mimetype='text/plain')\n",
    "        return \"Node not found\", 404\n",
    "\n",
    "    return app\n",
    "\n",
    "class GunicornApp(BaseApplication):\n",
    "    def __init__(self, app, options=None):\n",
    "        self.application = app\n",
    "        self.options = options or {}\n",
    "        super().__init__()\n",
    "\n",
    "    def load_config(self):\n",
    "        for key, value in self.options.items():\n",
    "            if key in self.cfg.settings and value is not None:\n",
    "                self.cfg.set(key, value)\n",
    "\n",
    "    def load(self):\n",
    "        return self.application\n",
    "\n",
    "def run_simulator(node_range, port):\n",
    "    app = create_flask_app(node_range)\n",
    "    options = {\n",
    "        'bind': f'127.0.0.1:{port}',\n",
    "        'workers': 4,\n",
    "    }\n",
    "    GunicornApp(app, options).run()\n",
    "\n",
    "# Kafka Producer Functions\n",
    "async def create_producer():\n",
    "    try:\n",
    "        producer = AIOKafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            key_serializer=lambda k: k.encode('utf-8') if k is not None else None\n",
    "        )\n",
    "        await producer.start()\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Kafka producer: {e}\")\n",
    "        raise\n",
    "\n",
    "async def shutdown_producer(producer):\n",
    "    logger.info(\"Shutting down producer...\")\n",
    "    await producer.stop()\n",
    "    logger.info(\"Producer shutdown complete.\")\n",
    "\n",
    "def parse_dcgm_metrics(metrics):\n",
    "    lines = metrics.strip().split('\\n')\n",
    "    parsed_metrics = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        try:\n",
    "            metric_name, rest = line.split('{', 1)\n",
    "            labels, value = rest.rsplit('}', 1)\n",
    "            value = value.strip()\n",
    "            metric_name = metric_name.strip()\n",
    "\n",
    "            label_dict = {}\n",
    "            for label in labels.split(','):\n",
    "                key, val = label.split('=')\n",
    "                label_dict[key.strip()] = val.strip('\"')\n",
    "\n",
    "            key = f\"{metric_name}_gpu{label_dict.get('gpu', 'unknown')}\"\n",
    "\n",
    "            message = {\n",
    "                \"metric_name\": metric_name,\n",
    "                \"value\": float(value),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"UUID\": str(uuid.uuid4()),\n",
    "                \"gpu\": label_dict.get('gpu', 'unknown'),\n",
    "                \"pci_bus_id\": label_dict.get('pci_bus_id', 'unknown'),\n",
    "                \"device\": label_dict.get('device', 'unknown'),\n",
    "                \"modelName\": label_dict.get('modelName', 'unknown'),\n",
    "                \"Hostname\": label_dict.get('Hostname', 'unknown'),\n",
    "                \"DCGM_FI_DRIVER_VERSION\": label_dict.get('DCGM_FI_DRIVER_VERSION', 'unknown'),\n",
    "                \"err_code\": \"N/A\",\n",
    "                \"err_msg\": \"N/A\"\n",
    "            }\n",
    "\n",
    "            parsed_metrics.append((key, message))\n",
    "        except ValueError:\n",
    "            logger.error(f\"Failed to parse DCGM line: {line}\")\n",
    "    return parsed_metrics\n",
    "\n",
    "async def fetch_metrics(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        response.raise_for_status()\n",
    "        return await response.text()\n",
    "\n",
    "async def fetch_and_send_metrics(producer, session, node_range, port, end_time):\n",
    "    messages_sent = 0\n",
    "    while datetime.now() < end_time:\n",
    "        for node_id in node_range:\n",
    "            url = f\"http://127.0.0.1:{port}/metrics/{node_id}\"\n",
    "            try:\n",
    "                metrics = await fetch_metrics(session, url)\n",
    "                parsed_metrics = parse_dcgm_metrics(metrics)\n",
    "                for attempt in range(RETRIES):\n",
    "                    try:\n",
    "                        for key, message in parsed_metrics:\n",
    "                            await producer.send_and_wait(DCGM_KAFKA_TOPIC, key=key, value=message)\n",
    "                            messages_sent += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to send metrics from node {node_id}, retrying... Attempt {attempt+1}/{RETRIES}\")\n",
    "                        await asyncio.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    logger.error(f\"Failed to send metrics from node {node_id} after multiple attempts.\")\n",
    "            except aiohttp.ClientError as e:\n",
    "                logger.error(f\"Error fetching metrics from node {node_id}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error processing node {node_id}: {e}\")\n",
    "        await asyncio.sleep(FETCH_INTERVAL)\n",
    "    return messages_sent\n",
    "\n",
    "# Kafka Consumer Functions\n",
    "async def create_consumer():\n",
    "    consumer = AIOKafkaConsumer(\n",
    "        DCGM_KAFKA_TOPIC,\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        group_id='benchmark-consumer-group',\n",
    "        auto_offset_reset='earliest'\n",
    "    )\n",
    "    await consumer.start()\n",
    "    return consumer\n",
    "\n",
    "async def shutdown_consumer(consumer):\n",
    "    logger.info(\"Shutting down consumer...\")\n",
    "    await consumer.stop()\n",
    "    logger.info(\"Consumer shutdown complete.\")\n",
    "\n",
    "async def consume_metrics(consumer, end_time):\n",
    "    metrics = {\n",
    "        'messages_received': 0,\n",
    "        'bytes_received': 0,\n",
    "        'processing_time': timedelta(0),\n",
    "        'errors': 0\n",
    "    }\n",
    "    start_time = datetime.now()\n",
    "    last_log_time = start_time\n",
    "\n",
    "    try:\n",
    "        async for msg in consumer:\n",
    "            if datetime.now() >= end_time:\n",
    "                break\n",
    "            metrics['messages_received'] += 1\n",
    "            metrics['bytes_received'] += len(msg.value)\n",
    "            try:\n",
    "                process_start = time.monotonic()\n",
    "                message_value = json.loads(msg.value.decode('utf-8'))\n",
    "                # Process message_value here if needed\n",
    "                process_end = time.monotonic()\n",
    "                metrics['processing_time'] += timedelta(seconds=process_end - process_start)\n",
    "            except json.JSONDecodeError:\n",
    "                metrics['errors'] += 1\n",
    "                logger.error(f\"Failed to decode message: {msg.value}\")\n",
    "            \n",
    "            # Periodic Logging (every minute)\n",
    "            now = datetime.now()\n",
    "            if (now - last_log_time).total_seconds() >= 60:\n",
    "                log_metrics(metrics, start_time, now)\n",
    "                last_log_time = now\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during consumption: {e}\")\n",
    "        metrics['errors'] += 1\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def log_metrics(metrics, start_time, current_time):\n",
    "    elapsed_time = current_time - start_time\n",
    "    throughput = metrics['messages_received'] / elapsed_time.total_seconds() if elapsed_time.total_seconds() > 0 else 0\n",
    "    logger.info(f\"[{current_time}] Messages Received: {metrics['messages_received']}, \"\n",
    "                f\"Throughput: {throughput:.2f} msg/s, Errors: {metrics['errors']}\")\n",
    "\n",
    "def log_summary(metrics, start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    throughput = metrics['messages_received'] / elapsed_time.total_seconds() if elapsed_time.total_seconds() > 0 else 0\n",
    "    avg_processing_time = metrics['processing_time'].total_seconds() / metrics['messages_received'] if metrics['messages_received'] > 0 else 0\n",
    "    logger.info(\"\\n--- Summary ---\")\n",
    "    logger.info(f\"Start Time: {start_time}\")\n",
    "    logger.info(f\"End Time: {end_time}\")\n",
    "    logger.info(f\"Elapsed Time: {elapsed_time}\")\n",
    "    logger.info(f\"Messages Received: {metrics['messages_received']}\")\n",
    "    logger.info(f\"Bytes Received: {metrics['bytes_received']}\")\n",
    "    logger.info(f\"Throughput: {throughput:.2f} msg/s\")\n",
    "    logger.info(f\"Average Processing Time: {avg_processing_time:.4f} s/msg\")\n",
    "    logger.info(f\"Errors: {metrics['errors']}\")\n",
    "\n",
    "async def run_benchmark(num_nodes):\n",
    "    num_processes = min(multiprocessing.cpu_count(), 16)\n",
    "    nodes_per_process = num_nodes // num_processes\n",
    "    remaining_nodes = num_nodes % num_processes\n",
    "\n",
    "    # Start GPU simulators\n",
    "    simulator_processes = []\n",
    "    for i in range(num_processes):\n",
    "        start_node = i * nodes_per_process\n",
    "        end_node = start_node + nodes_per_process + (remaining_nodes if i == num_processes - 1 else 0)\n",
    "        node_range = range(start_node, end_node)\n",
    "        port = 51000 + i\n",
    "        p = multiprocessing.Process(target=run_simulator, args=(node_range, port))\n",
    "        simulator_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # Wait for simulators to start\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "    # Start Kafka producer\n",
    "    producer = await create_producer()\n",
    "    consumer = await create_consumer()\n",
    "    \n",
    "    end_time = datetime.now() + timedelta(seconds=BENCHMARK_DURATION)\n",
    "    \n",
    "    # Start consumer task\n",
    "    consumer_task = asyncio.create_task(consume_metrics(consumer, end_time))\n",
    "\n",
    "    # Start producer tasks\n",
    "    producer_tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in range(num_processes):\n",
    "            start_node = i * nodes_per_process\n",
    "            end_node = start_node + nodes_per_process + (remaining_nodes if i == num_processes - 1 else 0)\n",
    "            node_range = range(start_node, end_node)\n",
    "            port = 51000 + i\n",
    "            task = asyncio.create_task(fetch_and_send_metrics(producer, session, node_range, port, end_time))\n",
    "            producer_tasks.append(task)\n",
    "        \n",
    "        # Wait for producer tasks to complete\n",
    "        await asyncio.gather(*producer_tasks)\n",
    "\n",
    "    # Wait for consumer task to complete and get metrics\n",
    "    consumer_metrics = await consumer_task\n",
    "\n",
    "    # Shutdown Kafka producer and consumer\n",
    "    await shutdown_producer(producer)\n",
    "    await shutdown_consumer(consumer)\n",
    "\n",
    "    # Terminate simulator processes\n",
    "    for p in simulator_processes:\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "    # Log summary\n",
    "    log_summary(consumer_metrics, end_time - timedelta(seconds=BENCHMARK_DURATION), end_time)\n",
    "\n",
    "    return consumer_metrics['messages_received'] / BENCHMARK_DURATION\n",
    "\n",
    "async def main():\n",
    "    node_counts = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 5096]\n",
    "    results = {}\n",
    "\n",
    "    for num_nodes in node_counts:\n",
    "        logger.info(f\"Running benchmark for {num_nodes} nodes...\")\n",
    "        messages_per_second = await run_benchmark(num_nodes)\n",
    "        results[num_nodes] = messages_per_second\n",
    "        logger.info(f\"Benchmark result for {num_nodes} nodes: {messages_per_second:.2f} messages/second\")\n",
    "\n",
    "    logger.info(\"Final Benchmark Results:\")\n",
    "    for num_nodes, messages_per_second in results.items():\n",
    "        logger.info(f\"{num_nodes} nodes: {messages_per_second:.2f} messages/second\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8aff5a8-a00b-4f83-aac4-94fe316cdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_gpu_simulator.py\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import aiohttp\n",
    "import uuid\n",
    "import time\n",
    "import secrets\n",
    "import math\n",
    "import multiprocessing\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "from datetime import datetime, timedelta\n",
    "from flask import Flask, Response\n",
    "from gunicorn.app.base import BaseApplication\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['10.180.8.24:9092', '10.180.8.24:9093', '10.180.8.24:9094']\n",
    "DCGM_KAFKA_TOPIC = 'dcgm-metrics-test'\n",
    "FETCH_INTERVAL = 1  # Seconds\n",
    "BATCH_SIZE = 100  # Number of nodes to process in one batch\n",
    "MAX_CONCURRENT_REQUESTS = 50\n",
    "CONNECTION_TIMEOUT = 30\n",
    "REQUEST_TIMEOUT = 10\n",
    "\n",
    "@dataclass\n",
    "class GPUSpecs:\n",
    "    sm_clock: Tuple[int, int] = (1230, 1380)\n",
    "    mem_clock: int = 877\n",
    "    power: Tuple[int, int] = (300, 350)\n",
    "    memory: int = 16384\n",
    "    model: str = 'Tesla V100-SXM2-16GB'\n",
    "\n",
    "class GPUSimulator:\n",
    "    def __init__(self, node_id: int, gpu_id: int):\n",
    "        self.node_id = node_id\n",
    "        self.gpu_id = gpu_id\n",
    "        self.uuid = f\"GPU-{secrets.token_hex(16)}\"\n",
    "        self.pci_bus_id = f\"{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}.0\"\n",
    "        self.driver_version = \"450.51.06\"\n",
    "        self.specs = GPUSpecs()\n",
    "        # Use numpy for faster calculations\n",
    "        self.workload_phase = np.random.random()\n",
    "        self.noise = np.random.normal(0, 0.05, size=1000)  # Pre-generate noise\n",
    "        self.noise_idx = 0\n",
    "\n",
    "    def get_noise(self) -> float:\n",
    "        self.noise_idx = (self.noise_idx + 1) % len(self.noise)\n",
    "        return self.noise[self.noise_idx]\n",
    "\n",
    "    def simulate_metrics(self) -> List[Tuple[str, float, str]]:\n",
    "        self.workload_phase = (self.workload_phase + 0.1) % 1.0\n",
    "        workload = (np.sin(self.workload_phase * 2 * np.pi) + 1) / 2\n",
    "        workload = np.clip(workload + self.get_noise(), 0, 1)\n",
    "\n",
    "        sm_clock = int(self.specs.sm_clock[0] + (self.specs.sm_clock[1] - self.specs.sm_clock[0]) * workload)\n",
    "        gpu_temp = 30 + int(55 * workload)\n",
    "        power_usage = self.specs.power[0] + (self.specs.power[1] - self.specs.power[0]) * workload\n",
    "        gpu_util = int(100 * workload)\n",
    "        mem_util = int(90 * workload)\n",
    "        fb_used = self.specs.memory * mem_util / 100\n",
    "        fb_free = self.specs.memory - fb_used\n",
    "\n",
    "        return [\n",
    "            ('DCGM_FI_DEV_SM_CLOCK', sm_clock, 'SM clock frequency (in MHz)'),\n",
    "            ('DCGM_FI_DEV_GPU_TEMP', gpu_temp, 'GPU temperature (in C)'),\n",
    "            ('DCGM_FI_DEV_POWER_USAGE', round(power_usage, 2), 'Power draw (in W)'),\n",
    "            ('DCGM_FI_DEV_GPU_UTIL', gpu_util, 'GPU utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_MEM_COPY_UTIL', mem_util, 'Memory utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_FB_FREE', round(fb_free, 2), 'Frame buffer memory free (in MB)'),\n",
    "            ('DCGM_FI_DEV_FB_USED', round(fb_used, 2), 'Frame buffer memory used (in MB)')\n",
    "        ]\n",
    "\n",
    "class MetricsBatchProcessor:\n",
    "    def __init__(self, producer, batch_size=BATCH_SIZE):\n",
    "        self.producer = producer\n",
    "        self.batch_size = batch_size\n",
    "        self.batch = []\n",
    "\n",
    "    async def add_metrics(self, key: str, value: Dict):\n",
    "        self.batch.append((key, value))\n",
    "        if len(self.batch) >= self.batch_size:\n",
    "            await self.flush()\n",
    "\n",
    "    async def flush(self):\n",
    "        if not self.batch:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Use gather with return_exceptions=True to handle partial failures\n",
    "            tasks = [\n",
    "                self.producer.send_and_wait(DCGM_KAFKA_TOPIC, key=key, value=value)\n",
    "                for key, value in self.batch\n",
    "            ]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            # Handle any exceptions\n",
    "            for i, result in enumerate(results):\n",
    "                if isinstance(result, Exception):\n",
    "                    logging.error(f\"Failed to send message {i}: {result}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Batch processing error: {e}\")\n",
    "        finally:\n",
    "            self.batch.clear()\n",
    "\n",
    "async def benchmark_worker(worker_id: int, num_nodes: int, port: int) -> Dict:\n",
    "    metrics = {\n",
    "        'messages_processed': 0,\n",
    "        'errors': 0,\n",
    "        'processing_time': 0\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as session:\n",
    "        start_node = worker_id * (num_nodes // multiprocessing.cpu_count())\n",
    "        end_node = start_node + (num_nodes // multiprocessing.cpu_count())\n",
    "        \n",
    "        for node_id in range(start_node, end_node):\n",
    "            try:\n",
    "                url = f\"http://127.0.0.1:{port}/metrics/{node_id}\"\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 200:\n",
    "                        metrics['messages_processed'] += 1\n",
    "                    else:\n",
    "                        metrics['errors'] += 1\n",
    "            except Exception as e:\n",
    "                metrics['errors'] += 1\n",
    "                logging.error(f\"Worker {worker_id} error processing node {node_id}: {e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "async def run_benchmark(num_nodes: int, duration: int = 60) -> Dict:\n",
    "    num_workers = min(multiprocessing.cpu_count(), 16)\n",
    "    ports = list(range(51000, 51000 + num_workers))\n",
    "    \n",
    "    # Start simulators\n",
    "    simulator_processes = []\n",
    "    for i, port in enumerate(ports):\n",
    "        node_range = range(i * (num_nodes // num_workers), (i + 1) * (num_nodes // num_workers))\n",
    "        p = multiprocessing.Process(target=run_simulator, args=(node_range, port))\n",
    "        simulator_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    await asyncio.sleep(5)  # Wait for simulators to start\n",
    "\n",
    "    # Run benchmark\n",
    "    tasks = [\n",
    "        benchmark_worker(i, num_nodes, port)\n",
    "        for i, port in enumerate(ports)\n",
    "    ]\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Cleanup\n",
    "    for p in simulator_processes:\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "    # Aggregate results\n",
    "    total_metrics = {\n",
    "        'messages_processed': sum(r['messages_processed'] for r in results),\n",
    "        'errors': sum(r['errors'] for r in results),\n",
    "        'processing_time': sum(r['processing_time'] for r in results)\n",
    "    }\n",
    "\n",
    "    return total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a444b9-6f14-483c-827f-0ca067f41da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 243\u001b[0m\n\u001b[1;32m    240\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(result_line)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import aiohttp\n",
    "import uuid\n",
    "import time\n",
    "import secrets\n",
    "import math\n",
    "import multiprocessing\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "from datetime import datetime, timedelta\n",
    "from flask import Flask, Response\n",
    "from gunicorn.app.base import BaseApplication\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['10.180.8.24:9092', '10.180.8.24:9093', '10.180.8.24:9094']\n",
    "DCGM_KAFKA_TOPIC = 'dcgm-metrics-test'\n",
    "FETCH_INTERVAL = 1  # Seconds\n",
    "RETRIES = 3\n",
    "RETRY_DELAY = 5  # Seconds\n",
    "MAX_NODES = 8\n",
    "GPUS_PER_NODE = 4\n",
    "GPU_MODEL = 'Tesla V100-SXM2-16GB'\n",
    "GPU_SPECS = {\n",
    "    'sm_clock': (1230, 1380),\n",
    "    'mem_clock': 877,\n",
    "    'power': (300, 350),\n",
    "    'memory': 16384\n",
    "}\n",
    "BENCHMARK_DURATION = 600  # 10 minutes in seconds\n",
    "LOG_FILE = \"benchmark_results.log\"\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup file handler for logging results to file\n",
    "file_handler = logging.FileHandler(LOG_FILE)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "class GPUSimulator:\n",
    "    def __init__(self, node_id, gpu_id):\n",
    "        self.node_id = node_id\n",
    "        self.gpu_id = gpu_id\n",
    "        self.uuid = f\"GPU-{secrets.token_hex(16)}\"\n",
    "        self.pci_bus_id = f\"{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}:{secrets.randbelow(256):02X}.0\"\n",
    "        self.driver_version = \"450.51.06\"\n",
    "        self.start_time = time.time()\n",
    "        self.workload_phase = secrets.randbelow(600) / 600\n",
    "\n",
    "    def simulate_metrics(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        self.workload_phase = ((elapsed_time / 60 + self.workload_phase) % 10) / 10\n",
    "        workload = (math.sin(self.workload_phase * 2 * math.pi) + 1) / 2\n",
    "        workload = min(1, max(0, workload + (secrets.randbelow(20) - 10) / 100))\n",
    "        sm_clock = int(GPU_SPECS['sm_clock'][0] + (GPU_SPECS['sm_clock'][1] - GPU_SPECS['sm_clock'][0]) * workload)\n",
    "        gpu_temp = 30 + int(55 * workload)\n",
    "        power_usage = GPU_SPECS['power'][0] + (GPU_SPECS['power'][1] - GPU_SPECS['power'][0]) * workload\n",
    "        gpu_util = int(100 * workload)\n",
    "        mem_util = int(90 * workload)\n",
    "        fb_used = GPU_SPECS['memory'] * mem_util / 100\n",
    "        fb_free = GPU_SPECS['memory'] - fb_used\n",
    "        return [\n",
    "            ('DCGM_FI_DEV_SM_CLOCK', sm_clock, 'SM clock frequency (in MHz)'),\n",
    "            ('DCGM_FI_DEV_GPU_TEMP', gpu_temp, 'GPU temperature (in C)'),\n",
    "            ('DCGM_FI_DEV_POWER_USAGE', round(power_usage, 2), 'Power draw (in W)'),\n",
    "            ('DCGM_FI_DEV_GPU_UTIL', gpu_util, 'GPU utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_MEM_COPY_UTIL', mem_util, 'Memory utilization (in %)'),\n",
    "            ('DCGM_FI_DEV_FB_FREE', round(fb_free, 2), 'Frame buffer memory free (in MB)'),\n",
    "            ('DCGM_FI_DEV_FB_USED', round(fb_used, 2), 'Frame buffer memory used (in MB)')\n",
    "        ]\n",
    "\n",
    "def generate_gpu_metrics(node_id, gpu_simulator):\n",
    "    metrics = gpu_simulator.simulate_metrics()\n",
    "    formatted_metrics = []\n",
    "    for name, value, help_text in metrics:\n",
    "        formatted_metrics.extend([\n",
    "            f'# HELP {name} {help_text}',\n",
    "            f'# TYPE {name} gauge',\n",
    "            f'{name}{{node=\"{node_id}\",gpu=\"{gpu_simulator.gpu_id}\",UUID=\"{gpu_simulator.uuid}\",'\n",
    "            f'pci_bus_id=\"{gpu_simulator.pci_bus_id}\",device=\"nvidia{gpu_simulator.gpu_id}\",'\n",
    "            f'modelName=\"{GPU_MODEL}\",Hostname=\"node{node_id:04d}\",'\n",
    "            f'DCGM_FI_DRIVER_VERSION=\"{gpu_simulator.driver_version}\"}} {value}'\n",
    "        ])\n",
    "    return '\\n'.join(formatted_metrics)\n",
    "\n",
    "def get_gpu_metrics(node_id):\n",
    "    gpu_simulators = [GPUSimulator(node_id, gpu_id) for gpu_id in range(GPUS_PER_NODE)]\n",
    "    all_metrics = []\n",
    "    for gpu_simulator in gpu_simulators:\n",
    "        all_metrics.append(generate_gpu_metrics(node_id, gpu_simulator))\n",
    "    return '\\n'.join(all_metrics)\n",
    "\n",
    "def create_flask_app(node_range):\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/metrics/<int:node_id>', methods=['GET'])\n",
    "    def metrics(node_id):\n",
    "        if node_id in node_range:\n",
    "            metrics_data = get_gpu_metrics(node_id)\n",
    "            return Response(metrics_data, mimetype='text/plain')\n",
    "        return \"Node not found\", 404\n",
    "\n",
    "    return app\n",
    "\n",
    "class GunicornApp(BaseApplication):\n",
    "    def __init__(self, app, options=None):\n",
    "        self.application = app\n",
    "        self.options = options or {}\n",
    "        super().__init__()\n",
    "\n",
    "    def load_config(self):\n",
    "        for key, value in self.options.items():\n",
    "            if key in self.cfg.settings and value is not None:\n",
    "                self.cfg.set(key, value)\n",
    "\n",
    "    def load(self):\n",
    "        return self.application\n",
    "\n",
    "def run_simulator(node_range, port):\n",
    "    app = create_flask_app(node_range)\n",
    "    options = {\n",
    "        'bind': f'127.0.0.1:{port}',\n",
    "        'workers': 4,\n",
    "    }\n",
    "    GunicornApp(app, options).run()\n",
    "\n",
    "async def create_producer():\n",
    "    try:\n",
    "        producer = AIOKafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            key_serializer=lambda k: k.encode('utf-8') if k is not None else None\n",
    "        )\n",
    "        await producer.start()\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Kafka producer: {e}\")\n",
    "        raise\n",
    "\n",
    "async def shutdown_producer(producer):\n",
    "    logger.info(\"Shutting down producer...\")\n",
    "    await producer.stop()\n",
    "    logger.info(\"Producer shutdown complete.\")\n",
    "\n",
    "async def fetch_metrics(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        response.raise_for_status()\n",
    "        return await response.text()\n",
    "\n",
    "def log_summary(metrics, start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    throughput = metrics['messages_received'] / elapsed_time.total_seconds() if elapsed_time.total_seconds() > 0 else 0\n",
    "    avg_processing_time = metrics['processing_time'].total_seconds() / metrics['messages_received'] if metrics['messages_received'] > 0 else 0\n",
    "\n",
    "    summary = (\n",
    "        f\"\\n--- Summary ---\\n\"\n",
    "        f\"Start Time: {start_time}\\n\"\n",
    "        f\"End Time: {end_time}\\n\"\n",
    "        f\"Elapsed Time: {elapsed_time}\\n\"\n",
    "        f\"Messages Received: {metrics['messages_received']}\\n\"\n",
    "        f\"Bytes Received: {metrics['bytes_received']}\\n\"\n",
    "        f\"Throughput: {throughput:.2f} msg/s\\n\"\n",
    "        f\"Average Processing Time: {avg_processing_time:.4f} s/msg\\n\"\n",
    "        f\"Errors: {metrics['errors']}\\n\"\n",
    "    )\n",
    "    \n",
    "    logger.info(summary)\n",
    "    \n",
    "    # Write summary to a log file\n",
    "    with open(\"benchmark_summary.txt\", \"a\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "async def run_benchmark(num_nodes):\n",
    "    num_processes = min(multiprocessing.cpu_count(), 16)\n",
    "    nodes_per_process = num_nodes // num_processes\n",
    "    remaining_nodes = num_nodes % num_processes\n",
    "\n",
    "    simulator_processes = []\n",
    "    for i in range(num_processes):\n",
    "        start_node = i * nodes_per_process\n",
    "        end_node = start_node + nodes_per_process + (remaining_nodes if i == num_processes - 1 else 0)\n",
    "        node_range = range(start_node, end_node)\n",
    "        port = 51000 + i\n",
    "        p = multiprocessing.Process(target=run_simulator, args=(node_range, port))\n",
    "        simulator_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "    producer = await create_producer()\n",
    "    consumer = await create_consumer()\n",
    "    \n",
    "    end_time = datetime.now() + timedelta(seconds=BENCHMARK_DURATION)\n",
    "    \n",
    "    consumer_task = asyncio.create_task(consume_metrics(consumer, end_time))\n",
    "\n",
    "    producer_tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in range(num_processes):\n",
    "            start_node = i * nodes_per_process\n",
    "            end_node = start_node + nodes_per_process + (remaining_nodes if i == num_processes - 1 else 0)\n",
    "            node_range = range(start_node, end_node)\n",
    "            port = 51000 + i\n",
    "            task = asyncio.create_task(fetch_and_send_metrics(producer, session, node_range, port, end_time))\n",
    "            producer_tasks.append(task)\n",
    "        \n",
    "        await asyncio.gather(*producer_tasks)\n",
    "\n",
    "    consumer_metrics = await consumer_task\n",
    "\n",
    "    await shutdown_producer(producer)\n",
    "    await shutdown_consumer(consumer)\n",
    "\n",
    "    for p in simulator_processes:\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "    log_summary(consumer_metrics, end_time - timedelta(seconds=BENCHMARK_DURATION), end_time)\n",
    "\n",
    "    return consumer_metrics['messages_received'] / BENCHMARK_DURATION\n",
    "\n",
    "async def main():\n",
    "    node_counts = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 5096]\n",
    "    results = {}\n",
    "\n",
    "    for num_nodes in node_counts:\n",
    "        logger.info(f\"Running benchmark for {num_nodes} nodes...\")\n",
    "        messages_per_second = await run_benchmark(num_nodes)\n",
    "        results[num_nodes] = messages_per_second\n",
    "        logger.info(f\"Benchmark result for {num_nodes}: {messages_per_second:.2f} messages/second\")\n",
    "\n",
    "    logger.info(\"Final Benchmark Results:\")\n",
    "    with open(\"benchmark_summary.txt\", \"a\") as f:\n",
    "        for num_nodes, messages_per_second in results.items():\n",
    "            result_line = f\"{num_nodes} nodes: {messages_per_second:.2f} messages/second\\n\"\n",
    "            logger.info(result_line)\n",
    "            f.write(result_line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e169b-b7d5-4116-8b5f-6d017a8a2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import multiprocessing\n",
    "\n",
    "def check_node(node_id, base_port):\n",
    "    url = f\"http://localhost:{base_port + node_id}/metrics\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return node_id, True, \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return node_id, False, str(e)\n",
    "\n",
    "def main():\n",
    "    num_nodes = 1024  # Replace with your number of nodes\n",
    "    base_port = 51000  # Replace with your base port\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:  # Use all CPU cores\n",
    "        results = pool.starmap(check_node, [(node_id, base_port) for node_id in range(num_nodes)])\n",
    "\n",
    "    down_servers = []\n",
    "    all_nodes_ok = True\n",
    "    for node_id, is_up, error_message in results:\n",
    "        if not is_up:\n",
    "            all_nodes_ok = False\n",
    "            down_servers.append(f\"Node {node_id}: {error_message}\")\n",
    "\n",
    "    if all_nodes_ok:\n",
    "        print(\"\\nAll nodes are running and responding.\")\n",
    "    else:\n",
    "        print(\"\\nThe following nodes are not running or responding:\")\n",
    "        for server in down_servers:\n",
    "            print(server)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70ceb1-124f-49f2-956c-2ee3d7d98485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
